# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/150qID4aoHIGGAyJt73bmb2rkMHbMfPRA
"""

# 下载数据
#上传文件
#from google.colab import files
#upload = files.upload()
from gensim.models.doc2vec import Doc2Vec

"""# 数据加载"""

import pandas as pd
data = pd.read_csv("chinese_news.csv")
data = data.dropna()
#print(data[560:570]


"""新闻文本"""

documents = data['content']#[1:10]
print("Load data done.")

doc_num = len(documents)
print(doc_num)

"""# 分词"""

import jieba
import jieba.analyse
import tqdm

#加载停用词
def getStopwords():
    stopwords = []
    with open("stopwords.txt", "r", encoding='utf8') as f:
        lines = f.readlines()
        for line in lines:
            stopwords.append(line.strip())
    return stopwords

stopwords = getStopwords()

#分词

import re
import logging


def segment():
    file_nums = 0
    count = 0
    texts = []
    news_texts = []
    for document in documents:
      doc_words = []
      text = re.split('(。|！|\!|\.|？|\?)',document) #分句
      for sentence in text:
        sentence = list(jieba.cut(sentence.strip().encode('utf-8')))
        sentence_segment = []
        for word in sentence:
          if word not in stopwords:
            sentence_segment.append(word)
        if len(sentence_segment) != 0:
          texts.append(sentence_segment)
          doc_words = doc_words + sentence_segment
        logging.info('finished ' + str(file_nums) + 'file word Segmentation')
        file_nums += 1
      news_texts.append(doc_words)
    return texts,news_texts

word_segments,news_texts = segment()

"""# 词频统计"""

from collections import defaultdict

frequency = defaultdict(int)

#遍历分词结果 计算每个词出现的频率
for text in word_segments:
  for token in text:
    frequency[token] +=1
    
text_segments =[]
for text in word_segments:
    tmp_text = []
    for word in text:
        if frequency[word] >1:
            tmp_text.append(word)
    #if len(tmp_text) != 0:
    text_segments.append(tmp_text)
#print(word_segments)


#DATA_PATH = os.path.join(ROOT_DIR, "data")

#保存分词结果

#segmented_file = os.path.join('','segmented')
segmented_file = open('segmented', "w", encoding="utf-8")
for text in text_segments:
  st = ""
#  #print(text)
  for token in text:
    st = st + token
    st = st + " "
   #print(st)
  segmented_file.write(st)
  segmented_file.flush()

import re
def segment_doc(doc):
  sents = re.split('(。|！|\!|\.|？|\?)',doc) #分句
  doc_tokens = []
  for sentence in sents:
    sentence = list(jieba.cut(sentence.strip().encode('utf-8')))
    sentence_segment = []
    for word in sentence:
      if word not in stopwords:
        sentence_segment.append(word)
    if len(sentence_segment) != 0:
      doc_tokens = doc_tokens + sentence_segment
  return doc_tokens

"""# 词云"""


# %matplotlib inline

def show_words_cloud(text_tokens):
  from wordcloud import WordCloud, ImageColorGenerator
  import matplotlib.pyplot as plt

  wc = WordCloud( background_color='white',
               width=640,
               height=480,
               font_path='SourceHanSansCN-Regular.otf').generate(" ".join(text_tokens))

  plt.figure(figsize=[12, 8])
  plt.imshow(wc)
  plt.axis('off')

"""# Doc2Vec"""
from gensim.test.utils import get_tmpfile
fname = get_tmpfile("doc2vec_model")
doc2vec_model = Doc2Vec.load(fname)


def News2vec():
  news_vectors = []
  for news_token in news_texts:
    vector = doc2vec_model.infer_vector(news_token) 
    news_vectors.append(vector)
  return news_vectors
  
news_vectors = News2vec()
#test
"""
#test_text = "中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015年12月31日在八一大楼隆重举行。中共中央总书记、国家主席、中央军委主席习近平向陆军、火箭军、战略支援部队授予军旗并致训词，代表党中央和中央军委向同志们、向全军部队致以热烈祝贺，强调要坚持以党在新形势下的强军目标为引领，深入贯彻新形势下军事战略方针，全面实施改革强军战略，坚定不移走中国特色强军之路，时刻听从党和人民召唤，忠实履行党和人民赋予的神圣使命，为实现中国梦强军梦作出新的更大的贡献。\n下午4时，成立大会开始，全场高唱国歌。仪仗礼兵护卫着鲜艳军旗，正步行进到主席台前。习近平将军旗郑重授予陆军司令员李作成、政治委员刘雷，火箭军司令员魏凤和政治委员王家胜，战略支援部队司令员高津、政治委员刘福连。陆军、火箭军、战略支援部队主要领导，军容严整、精神抖擞，向习近平敬礼，从习近平手中接过军旗。全场官兵向军旗敬礼。\n授旗仪式后，习近平致训词。他指出：“成立陆军领导机构、火箭军、战略支援部队，是党中央和中央军委着眼实现中国梦强军梦作出的重大决策，是构建中国特色现代军事力量体系的战略举措，必将成为我军现代化建设的一个重要里程碑，载入人民军队史册。”\n习近平强调，陆军是党最早建立和领导的武装力量，历史悠久，敢打善战，战功卓著，为党和人民建立了不朽功勋。陆军对维护国家主权、安全和发展利益具有不可替代的作用。陆军全体官兵要弘扬陆军光荣传统和优良作风，适应信息化时代陆军建设模式和运用方式的深刻变化，探索陆军发展特点和规律，按照机动作战、立体攻防的战略要求，加强顶层设计和领导管理，优化力量结构和部队编成，加快实现区域防卫型向全域作战型转变，努力建设一支强大的现代化新型陆军。\n习近平强调，火箭军是我国战略威慑的核心力量，是我国大国地位的战略支撑，是维护国家安全的重要基石。火箭军全体官兵要把握火箭军的职能定位和使命任务，按照核常兼备、全域慑战的战略要求，增强可信可靠的核威慑和核反击能力，加强中远程精确打击力量建设，增强战略制衡能力，努力建设一支强大的现代化火箭军。\n习近平强调，战略支援部队是维护国家安全的新型作战力量，是我军新质作战能力的重要增长点。战略支援部队全体官兵要坚持体系融合、军民融合，努力在关键领域实现跨越发展，高标准高起点推进新型作战力量加速发展、一体发展，努力建设一支强大的现代化战略支援部队。
"""

#news_token = segment_doc(test_text)
#vector = doc2vec_model.infer_vector(news_token)
#text_tokens = segment_doc(test_text) 
#sims = doc2vec_model.docvecs.most_similar([vector], topn=10) 
#for item in range(len(sims)):
#   print(documents[item],sims[item][1])

from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile
from gensim.similarities import Similarity
from gensim import corpora,models,similarities

corpora_documents = []
for document in documents:
  tokens = []
  #sents = re.split('(。|！|\!|\.|？|\?)',document) #分句
  sents = jieba.lcut(document) 
  sents_segment = []
  for word in sents:
          if word not in stopwords:
            sents_segment.append(word)
  corpora_documents.append(sents_segment)
dictionary = corpora.Dictionary(corpora_documents)
corpus = [dictionary.doc2bow(text) for text in corpora_documents]
#similarity = Similarity('temp', corpus, num_features=400)

#test
#test_cut_raw_1 = jieba.lcut(test_data_1)
#test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)
#
#int(test_corpus_1)
#similarity.num_best = 5
#print(similarity[test_corpus_1])

import time
from gensim import corpora,models,similarities
import pandas as pd
import numpy as np
dictionary = corpora.Dictionary(text_segments)
corpus = [dictionary.doc2bow(text) for text in text_segments]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
#LSI模型 抽取主题词
num_show_topic = 20
lsi = models.lsimodel.LsiModel.load('news_lsi.model')
topic_list=lsi.print_topics(num_topics=num_show_topic,num_words=20)
#所有文档的主题分布
corpus_lsi = lsi[corpus_tfidf]

lda = models.ldamodel.LdaModel.load('news_lda.model')
corpus_lda = lda[corpus_tfidf]

#for topic in topic_list:
idx = np.arange(doc_num)
np.random.shuffle(idx)
idx = idx[:10]
for i in idx:
        topic = np.array(corpus_lda[i])
        topic_distribute = np.array(topic[:, 1])
        # print topic_distribute
        topic_idx = topic_distribute.argsort()[:-num_show_topic-1:-1]
        print( ('第%d个文档的前%d个主题：' % (i, num_show_topic)), topic_idx)
        print(topic_distribute[topic_idx])

num_show_term = 8  
print('-----每个主题的词分布-----')
for topic_id in range(200):
        topic_words = []
        print('主题#%d：' % topic_id)
        term_distribute_all = lda.get_topic_terms(topicid=topic_id)
        term_distribute = term_distribute_all[:num_show_term]
        term_distribute = np.array(term_distribute)
        term_id = term_distribute[:, 0].astype(np.int)
        for t in term_id:
            print(dictionary.id2token[t])
            topic_words.append(dictionary.id2token[t])
        print('概率: ', term_distribute[:, 1])

#index = similarities.MatrixSimilarity(corpus_lsi)

#test_text = "中央军委印发《关于深化国防和军队改革的意见》"
#由词index，词频 构建的文档向量
#text_tokens = segment_doc(test_text)
#print(text_tokens)
#test_vec = dictionary.doc2bow(text_tokens)
#print(test_vec)
#计算与数据库中每条标题的相似度
#new_vec = index[test_vec]
#print(new_vec)

"""# 聚类"""

import numpy as np
from sklearn.cluster import KMeans
news_vecs = np.array(news_vectors)
def cluster(news_vecs):
  
  np.set_printoptions(threshold=np.inf)
  kmean = KMeans(n_clusters=300)
  print("kmeans",kmean.fit(news_vecs))
  #print(kmean.cluster_centers_)
  #print(len(kmean.labels_))
  return kmean.labels_,kmean.cluster_centers_
  
labels,centers = cluster(news_vecs)

"""聚类结果存储"""

fo = open("cluster.txt", "a+",encoding="utf8")
print(len(labels),len(documents))
null_data = [30, 563, 576, 578, 590, 641, 671, 709, 875, 878, 885, 944, 1181, 1396, 1397, 1585, 1689, 1692, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 2079, 2460, 2669, 2833, 3163, 3227, 3238, 3290, 3355, 3568, 3569, 3570, 3571, 3572, 4150, 4658, 4787, 4930, 5467, 5468, 5469, 5561, 5855, 6147, 6877, 7410, 7612, 8841, 10039, 10321, 10328, 10919, 11057, 11252, 12880, 12920, 12973, 12974, 12980, 13106, 13107, 13192, 13196, 13237, 13278, 13279, 13280, 13281, 13283, 13284, 13285, 13287, 13288, 13289, 13304, 13305, 13309, 13311, 13313, 13315, 13317, 13319, 13321, 13324, 13325, 13334, 13613, 13705, 14574, 14758, 14760, 14959, 16270, 17251, 17484, 17542, 17551, 18105, 19952, 20141, 20283]
for i in range(len(labels)):
  #print(i)
  #print(labels[i],' ',len(documents),"\n")
  if i not in null_data:
    try:
      fo.write(str(float(labels[i]))+" "+documents[i]+"\n")
    except:
      null_data.append(i)
fo.close()
print(null_data)

"""按聚类结果分割数据集"""

cluster_res = pd.DataFrame(labels, columns=['label'])
cluster_res = pd.concat([cluster_res, pd.DataFrame(documents)], axis=1)

cluster_res = cluster_res.sort_values('label')
#cluster_res.head()

#from google.colab import files
for label in range(300):
  print("Cluster#",label)
  cluster_segment = []
  cluster_tmp = cluster_res[cluster_res['label']==label]
  filename = "cluster_"+str(label)
  cluster_tmp.to_csv(filename, index=False, header=False)
  #dowload= files.download(filename)
  docs = cluster_tmp['content']
  for doc in docs:
    #print(doc)
    doc_tokens =  segment_doc(doc) 
    cluster_segment = cluster_segment + doc_tokens
  show_words_cloud(cluster_segment)

id_list = []
for item in range(len(corpus_lsi)):
  #print(item)
  #print(corpus_lsi[item])
  id_list.append(item)

#from gensim import matutils
#lda_csc_matrix = matutils.corpus2csc(corpus_lsi).transpose()  # gensim sparse matrix to scipy sparse matrix
#post_cluster(id_list, lda_csc_matrix)

"""# 年度新闻主题统计"""





"""模型加载

# 文档相似度计算
"""



"""# 命名实体识别"""

import tensorflow as tf
from tensorflow.python.ops import lookup_ops
import numpy as np
import collections
import os



def build_word_index():
  if not os.path.exists(word_embedding_file):
    print( 'word embedding file does not exist')
    return 
  
  print("Word Embedding....")
  
  if not os.path.exists(src_vocab_file):
    with open(src_vocab_file,'w') as source:
      f = open(word_embedding_file,'r')
      for line in f:
        values = line.split()
        word = values[0] #取词
