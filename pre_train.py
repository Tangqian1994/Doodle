# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/150qID4aoHIGGAyJt73bmb2rkMHbMfPRA
"""

# 下载数据
#上传文件
#from google.colab import files
#upload = files.upload()


"""# 数据加载"""

import pandas as pd
data = pd.read_csv("chinese_news.csv")
data = data.dropna()
#print(data[560:570]


"""新闻文本"""

documents = data['content']#[1:10]
print("Load data done.")

doc_num = len(documents)
print(doc_num)

"""# 分词"""

import jieba
import jieba.analyse
import tqdm

#加载停用词
def getStopwords():
    stopwords = []
    with open("stopwords.txt", "r", encoding='utf8') as f:
        lines = f.readlines()
        for line in lines:
            stopwords.append(line.strip())
    return stopwords

stopwords = getStopwords()

#分词

import re
import logging


def segment():
    file_nums = 0
    count = 0
    texts = []
    news_texts = []
    for document in documents:
      doc_words = []
      text = re.split('(。|！|\!|\.|？|\?)',document) #分句
      for sentence in text:
        sentence = list(jieba.cut(sentence.strip().encode('utf-8')))
        sentence_segment = []
        for word in sentence:
          if word not in stopwords:
            sentence_segment.append(word)
        if len(sentence_segment) != 0:
          texts.append(sentence_segment)
          doc_words = doc_words + sentence_segment
        logging.info('finished ' + str(file_nums) + 'file word Segmentation')
        file_nums += 1
      news_texts.append(doc_words)
    return texts,news_texts

word_segments,news_texts = segment()

"""# 词频统计"""

from collections import defaultdict

frequency = defaultdict(int)

#遍历分词结果 计算每个词出现的频率
for text in word_segments:
  for token in text:
    frequency[token] +=1
    
text_segments =[]
for text in word_segments:
    tmp_text = []
    for word in text:
        if frequency[word] >1:
            tmp_text.append(word)
    if len(tmp_text) != 0:
        text_segments.append(tmp_text)
#print(word_segments)


#DATA_PATH = os.path.join(ROOT_DIR, "data")

#保存分词结果

#segmented_file = os.path.join('','segmented')
segmented_file = open('segmented', "w", encoding="utf-8")
for text in text_segments:
  st = ""
#  #print(text)
  for token in text:
    st = st + token
    st = st + " "
   #print(st)
  segmented_file.write(st)
  segmented_file.flush()

print("Pre-procedding done.")
"""# Word Embedding"""
''''
#Wording Embedding
from gensim import corpora,models,similarities
from gensim.models import word2vec
import os



#print(dictionary.token2id)
print("Training Word2vec model....")
#sentences = word2vec.Text8Corpus(segmented_file)
model = word2vec.Word2Vec(text_segments,size=200,window=10,workers=8,sg=1,hs=1,iter=10)

word2vec_model_file = os.path.join('./','word2vec.model')

model.wv.save_word2vec_format(word2vec_model_file,binary=True)

print("Word2vec Model Saved")

"""测试模型"""

# 测试模型
print("test....")
from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format(word2vec_model_file, binary=True)
#print(query_list)
print("计算2个词汇间的 Cosine 相似度")
query_list = word_vectors.doesnt_match(['中国', '美国','旧金山'])
print(word_vectors.similarity('总书记','中国'))
print(query_list)
'''
"""# 文本向量生成"""

#Doc2Vec 模型
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.test.utils import get_tmpfile
docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(news_texts)]

print("Training Doc2vec.....")
doc2vec_model = Doc2Vec(docs,dm = 0, alpha=0.025, vector_size = 300, min_alpha=0.025,window=10,works=11)
fname = get_tmpfile("doc2vec_model")
doc2vec_model.save(fname)
print("Train finished")

"""模型导入"""

import re
def segment_doc(doc):
  sents = re.split('(。|！|\!|\.|？|\?)',doc) #分句
  doc_tokens = []
  for sentence in sents:
    sentence = list(jieba.cut(sentence.strip().encode('utf-8')))
    sentence_segment = []
    for word in sentence:
      if word not in stopwords:
        sentence_segment.append(word)
    if len(sentence_segment) != 0:
      doc_tokens = doc_tokens + sentence_segment
  return doc_tokens

"""# 词云"""


# %matplotlib inline

def show_words_cloud(text_tokens):
  from wordcloud import WordCloud, ImageColorGenerator
  import matplotlib.pyplot as plt

  wc = WordCloud( background_color='white',
               width=640,
               height=480,
               font_path='SourceHanSansCN-Regular.otf').generate(" ".join(text_tokens))

  plt.figure(figsize=[12, 8])
  plt.imshow(wc)
  plt.axis('off')

"""# Doc2Vec"""

doc2vec_model = Doc2Vec.load(fname)


def News2vec():
  news_vectors = []
  for news_token in news_texts:
    vector = doc2vec_model.infer_vector(news_token) 
    news_vectors.append(vector)
  return news_vectors
  
news_vectors = News2vec()
#test
"""
#test_text = "中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015年12月31日在八一大楼隆重举行。中共中央总书记、国家主席、中央军委主席习近平向陆军、火箭军、战略支援部队授予军旗并致训词，代表党中央和中央军委向同志们、向全军部队致以热烈祝贺，强调要坚持以党在新形势下的强军目标为引领，深入贯彻新形势下军事战略方针，全面实施改革强军战略，坚定不移走中国特色强军之路，时刻听从党和人民召唤，忠实履行党和人民赋予的神圣使命，为实现中国梦强军梦作出新的更大的贡献。\n下午4时，成立大会开始，全场高唱国歌。仪仗礼兵护卫着鲜艳军旗，正步行进到主席台前。习近平将军旗郑重授予陆军司令员李作成、政治委员刘雷，火箭军司令员魏凤和政治委员王家胜，战略支援部队司令员高津、政治委员刘福连。陆军、火箭军、战略支援部队主要领导，军容严整、精神抖擞，向习近平敬礼，从习近平手中接过军旗。全场官兵向军旗敬礼。\n授旗仪式后，习近平致训词。他指出：“成立陆军领导机构、火箭军、战略支援部队，是党中央和中央军委着眼实现中国梦强军梦作出的重大决策，是构建中国特色现代军事力量体系的战略举措，必将成为我军现代化建设的一个重要里程碑，载入人民军队史册。”\n习近平强调，陆军是党最早建立和领导的武装力量，历史悠久，敢打善战，战功卓著，为党和人民建立了不朽功勋。陆军对维护国家主权、安全和发展利益具有不可替代的作用。陆军全体官兵要弘扬陆军光荣传统和优良作风，适应信息化时代陆军建设模式和运用方式的深刻变化，探索陆军发展特点和规律，按照机动作战、立体攻防的战略要求，加强顶层设计和领导管理，优化力量结构和部队编成，加快实现区域防卫型向全域作战型转变，努力建设一支强大的现代化新型陆军。\n习近平强调，火箭军是我国战略威慑的核心力量，是我国大国地位的战略支撑，是维护国家安全的重要基石。火箭军全体官兵要把握火箭军的职能定位和使命任务，按照核常兼备、全域慑战的战略要求，增强可信可靠的核威慑和核反击能力，加强中远程精确打击力量建设，增强战略制衡能力，努力建设一支强大的现代化火箭军。\n习近平强调，战略支援部队是维护国家安全的新型作战力量，是我军新质作战能力的重要增长点。战略支援部队全体官兵要坚持体系融合、军民融合，努力在关键领域实现跨越发展，高标准高起点推进新型作战力量加速发展、一体发展，努力建设一支强大的现代化战略支援部队。
"""

#news_token = segment_doc(test_text)
#vector = doc2vec_model.infer_vector(news_token)
#text_tokens = segment_doc(test_text) 
#sims = doc2vec_model.docvecs.most_similar([vector], topn=10) 
#for item in range(len(sims)):
#   print(documents[item],sims[item][1])

from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile
from gensim.similarities import Similarity
from gensim import corpora,models,similarities
wf = open('news_vec','w')
corpora_documents = []
for document in documents:
  tokens = []
  #sents = re.split('(。|！|\!|\.|？|\?)',document) #分句
  sents = jieba.lcut(document) 
  sents_segment = []
  for word in sents:
          if word not in stopwords:
            sents_segment.append(word)
  corpora_documents.append(sents_segment)

dictionary = corpora.Dictionary(corpora_documents)
corpus = [dictionary.doc2bow(text) for text in corpora_documents]
#similarity = Similarity('temp', corpus, num_features=400)

#test
#test_cut_raw_1 = jieba.lcut(test_data_1)
#test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)
#print(test_corpus_1)
#similarity.num_best = 5
#print(similarity[test_corpus_1])
import pandas as pd
import numpy as np
import time
#LDA模型
t_start = time.time()
lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=200)
lda.save('news_lda.model')
print('Train LDA model finished: %.3f s' % (time.time() - t_start))

lda = models.ldamodel.LdaModel.load('news_lda.model')
lda_topic_list=lda.print_topics(num_topics=20,num_words=20)
#for topic in lda_topic_list:
 # print(topic)

#LSI模型 抽取主题词

from gensim import corpora,models,similarities
#创建字典 单词与编号之间的映射
dictionary = corpora.Dictionary(text_segments)
corpus = [dictionary.doc2bow(text) for text in text_segments]

tfidf = models.TfidfModel(corpus)
#由tf_idf 表示的文档向量
corpus_tfidf = tfidf[corpus]
 
t_start = time.time()
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)
lsi.save('news_lsi.model')
print('Train LSI model finished: %.3f s' % (time.time() - t_start))

num_show_topic = 20
lsi = models.lsimodel.LsiModel.load('news_lsi.model')
topic_list=lsi.print_topics(num_topics=num_show_topic,num_words=20)


"""# 命名实体识别"""

import tensorflow as tf
from tensorflow.python.ops import lookup_ops
import numpy as np
import collections
import os



def build_word_index():
  if not os.path.exists(word_embedding_file):
    print( 'word embedding file does not exist')
    return 
  
  print("Word Embedding....")
  
  if not os.path.exists(src_vocab_file):
    with open(src_vocab_file,'w') as source:
      f = open(word_embedding_file,'r')
      for line in f:
        values = line.split()
        word = values[0] #取词
